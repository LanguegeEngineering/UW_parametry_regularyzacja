# -*- coding: utf-8 -*-
"""Regularization and Dropout.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/dphi-official/Deep_Learning_Bootcamp/blob/master/Optimization_Techniques/Regularization_and_Dropout.ipynb

### Regularization

Regularization is a technique which makes slight modifications to the learning algorithm such that the model generalizes better. This in turn improves the modelâ€™s performance on the unseen data as well. 

Remember when we were adding more layers to the model (making it more complex) ? Adding more than required layers might also lead to overfitting. 

*By looking at the graph below can you guess, what should be an ideal model complexity.*

![perfect_point.png](https://drive.google.com/uc?export=view&id=1isDRyvRtMFPhC8RQHL9Puh-OoOm0z9FP)

Somethings in Deep Learning can't be learnt in the first go. Regularisation is one of them. It takes some time. So, dont be hard on yourself and let this absorb.

Here is how to use it in your neural networks.
```python
model.add(Dense(256,activation='relu', kernel_regularizer = 'l2'))
```

### Dropouts

Dropout is a regularization method that approximates training a large number of neural networks with different architectures in parallel. As mentioned, it is preferrably used when training a large neural network.
"""

from IPython.display import Image
Image(url="https://miro.medium.com/max/1200/1*iWQzxhVlvadk6VAJjsgXgg.png", width=800, height=500)

"""*By dropping a unit out, we mean temporarily removing it from the network, along with all its incoming and outgoing connections*

Dropout has the effect of making the training process noisy, forcing nodes within a layer to probabilistically take on more or less responsibility for the inputs.
This conceptualization suggests that perhaps dropout breaks-up situations where network layers co-adapt to correct mistakes from prior layers, in turn making the model more robust.

It can be used with most types of layers, such as dense fully connected layers, convolutional layers, and recurrent layers such as the long short-term memory network layer. Dropout may be implemented on any or all hidden layers in the network as well as the visible or input layer. 

`NOTE` : It is not used on the output layer.
"""

# Commented out IPython magic to ensure Python compatibility.
from tensorflow.keras.datasets import mnist
# import matplotlib for visualization
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
# %matplotlib inline

(X_train, y_train), (X_test, y_test) = mnist.load_data()

images = X_train[:3]
labels = y_train[:3]

f,ax = plt.subplots(nrows=1,ncols=3, figsize=(20,4))

for index,(img, ax) in enumerate(zip(images, ax)):
    
    ax.imshow(img,cmap='gray')
    ax.axis('off')
    ax.text(0.6,-2.0, f"Digit in the Image {labels[index]}", size=15, ha="center")
    
plt.show()

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.callbacks import TensorBoard
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras import regularizers
from datetime import datetime

###why did we reshaped ?? Question for you !

X_train = X_train.reshape(60000, 784)
X_test = X_test.reshape(10000, 784)

###what is to_categorical doing here ? Question for you !

y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

model = Sequential()
model.add(Dense(200, activation='relu', input_shape= (784,) ))
model.add(Dense(10, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.001), metrics=['accuracy'])

#logdir = "logs/" + datetime.now().strftime("%Y%m%d-%H%M%S")
#tensorboard_callback = TensorBoard(log_dir=logdir)

#es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)

# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

training_history = model.fit(
    X_train, # input
    y_train, # output
    batch_size=32,
    verbose=1, # Suppress chatty output; use Tensorboard instead
    epochs=10,
    validation_data=(X_test, y_test),
)

history_dict = training_history.history
history_dict.keys()

acc = history_dict['accuracy']
val_acc = history_dict['val_accuracy']
loss = history_dict['loss']
val_loss = history_dict['val_loss']

epochs = range(1, len(acc) + 1)

# "bo" is for "blue dot"
plt.plot(epochs, loss, 'bo', label='Training loss')
# b is for "solid blue line"
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()

plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')

plt.show()

"""This is just for demo purpose, dorpouts is majorly used in deep/wide neural networks. This is a very basic one. As you can see, the accuracy is increasing very slowly and steadily and the model is yet to converge it will take around 100 more epochs.

**Points to conclude with**

- Large weights in a neural network are a sign of a more complex network that has overfit the training data.
- Probabilistically dropping out nodes in the network is a simple and effective regularization method.
"""