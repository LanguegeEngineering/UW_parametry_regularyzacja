# -*- coding: utf-8 -*-
"""HiPlot_student.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1osTufKtFPRVX2g3l06CbyOqbCEzGsFUp
"""

!pip install hiplot

import torch
import numpy as np

dtype = torch.float
device = torch.device("cpu")

N, D_in, H, D_out = 16, 4, 3, 1

# Create random input and output data

x_numpy = np.array(    [[0., 0., 0., 1.],
                        [1., 0., 0., 1.],
                        [0., 1., 0., 1.],
                        [0., 0., 1., 1.],
                        [1., 1., 0., 1.],
                        [1., 0., 1., 1.],
                        [0., 1., 1., 1.],
                        [1., 1., 1., 1.],
                        [0., 0., 0., 0.],
                        [1., 0., 0., 0.],
                        [0., 1., 0., 0.],
                        [0., 0., 1., 0.],
                        [1., 1., 0., 0.],
                        [1., 0., 1., 0.],
                        [0., 1., 1., 0.],
                        [1., 1., 1., 0.]])

x = torch.from_numpy(x_numpy).float()
print(x)

y_numpy = np.array(     [[1.],
                         [1.],
                         [1.],
                         [1.],
                         [1.],
                         [1.],
                         [1.],
                         [1.],
                         [0.],
                         [0.],
                         [0.],
                         [0.],
                         [0.],
                         [0.],
                         [0.],
                         [0.]])

y = torch.from_numpy(y_numpy).float()

learning_rates = [1e-4, 1e-3, 1e-2]
H = [5, 10, 20]
activation_functions = [torch.nn.Tanh(), torch.nn.ReLU()]

parameters_list = []
loss_lists = []
for learning_rate in learning_rates:
  for h in H:
    for activ in activation_functions:
      model = torch.nn.Sequential(
      torch.nn.Linear(D_in, h),
      activ,
      torch.nn.Linear(h, D_out)
      )
      
      
      loss_fn = torch.nn.MSELoss(reduction='sum')
      optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
      loss_list = []
      for t in range(100):
        y_pred = model(x)
        loss = loss_fn(y_pred, y)

        loss_list.append(loss.item())
        di ={"epoch": t,
            "neurons": h,
            "activ": activ._get_name(),
            "learning_rate": learning_rate, 
            "loss": loss.item()}
        parameters_list.append(di)


        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

      loss_lists.append(loss_list)

import matplotlib.pyplot as plt

number_of_loss_list = 0
plt.plot(loss_lists[number_of_loss_list], label = 'loss')
plt.legend()
plt.show()
print("Loss in last iteration wherer lr = {}: {}".format(learning_rates[number_of_loss_list], loss_lists[number_of_loss_list][-1]))

import hiplot as hip
hip.Experiment.from_iterable(parameters_list).display()

"""### Zadania

Tutaj najważniejsze to pobawić się różnymi parametrami i zobaczyć jak wpływają na funkcję straty. Najlepiej spóbować:

* dodać zmienną liczbę neuronów w warstwie pośrednieć
* dodać różne funkcje aktywacji
* zmienić dane wyjściowe na trudniejsze (np. copy-paste z pliku pytorch3)
"""

